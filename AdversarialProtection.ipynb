{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Защита модели от Adversarial примеров\n",
    "\n",
    "# ДАННАЯ РАБОТА РАССМАТРИВАЕТ НЕУДАЧНЫЙ ЭКСПЕРИМЕНТ\n",
    "\n",
    "Данная работа является продолжением предыдущей, в которой рассматривалось создание Adversarial примеров в задаче классификации изображений.\n",
    "\n",
    "Атака оказалась крайне удачной, и в ходе данной работы будут рассмотрены способы это исправить\n",
    "\n",
    "## Основные способы защиты, рассмотренные в работе:\n",
    "- [Ensemble adversarial training](https://arxiv.org/pdf/1705.07204) - модификация adversarial training, в которой adv. examples генерируются несколькими другими моделями\n",
    "- нормализация входных данных\n",
    "- использование небольшого dropout"
   ],
   "id": "4c963d313bb01bb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Для начала зададим все необходимые параметры",
   "id": "6b604bdaca89bfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:39:28.909587Z",
     "start_time": "2025-06-12T20:39:28.904509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "root   = Path(\"datasets/svhn_cls\")\n",
    "BATCH  = 128\n",
    "EPOCHS = 100\n",
    "BASE_LR = 3e-4\n",
    "EPS    = 1/255\n",
    "ADV_FRACTION = 0.25\n",
    "MODELS_PATH = Path(\"models\")\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 17\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device"
   ],
   "id": "53500154594501c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Для нормализации нам нужно посчитать среднее и дисперсию нашего датасета. Заметим, что они считаются только по обучающей выборке, чтобы тестовая выборка никак не влияла на процесс обучения",
   "id": "6f931dc9dc190252"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузим датасет, создадим трансформеры для него (с целью нормализации)",
   "id": "72e44148f3c3d665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:43:55.213396Z",
     "start_time": "2025-06-12T19:43:55.016750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "root = Path(\"datasets/svhn_cls\")\n",
    "mean = (0.4377, 0.4438, 0.4728)\n",
    "std  = (0.1201, 0.1231, 0.1052)\n",
    "\n",
    "tf_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "tf_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "ds_train = datasets.ImageFolder(root / \"train\", transform=tf_train)\n",
    "ds_test  = datasets.ImageFolder(root / \"test\",  transform=tf_test)\n",
    "dl_train = DataLoader(ds_train, batch_size=128, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "id": "2283077fb3ba63f0",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Создадим наш ансамбль моделей, которые будут генерировать adv. examples\n",
    "\n",
    "Для этого возьмём готовый resnet и за finetune-им его на нашем датасете (после этого представлен кусок кода для использования готовых дообученных моделей из [диска](https://disk.360.yandex.ru/d/PZcDdBJIB4_P8A)"
   ],
   "id": "f195cb920ae0a653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy, random, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# ─── helpers ────────────────────────────────────────────────────────────────────\n",
    "def eval_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x).argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# ─── main training routine ──────────────────────────────────────────────────────\n",
    "def train_resnet(seed: int,\n",
    "                 epochs: int = 4,\n",
    "                 lr: float   = 3e-4,\n",
    "                 wd: float   = 5e-4):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "    net = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "    net.fc = nn.Linear(net.fc.in_features, 10)\n",
    "    net.to(device)\n",
    "\n",
    "    opt   = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt, T_max=epochs * len(dl_train))\n",
    "\n",
    "    best_acc   = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        pbar = tqdm(dl_train, desc=f\"[seed={seed}] Epoch {ep:02d}/{epochs}\",\n",
    "                    leave=False)\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            F.cross_entropy(net(x), y).backward()\n",
    "            opt.step(); sched.step()\n",
    "\n",
    "        acc = eval_accuracy(net, dl_test)\n",
    "        print(f\"Epoch {ep:02d}: val accuracy = {acc*100:.2f}%\")\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_state = acc, copy.deepcopy(net.state_dict())\n",
    "            torch.save(\n",
    "                {\"epoch\":  ep,\n",
    "                 \"seed\":   seed,\n",
    "                 \"val_acc\":acc,\n",
    "                 \"state_dict\": best_state},\n",
    "                MODELS_PATH / f\"best_{seed}.pth\")\n",
    "\n",
    "    net.load_state_dict(best_state)\n",
    "    net.eval()\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    print(f\"Лучшая accurasy на сиде {seed}: {best_acc*100:.2f}%\\n\")\n",
    "    return net, best_acc\n",
    "\n",
    "resnet_A, acc_A = train_resnet(seed=42)\n",
    "resnet_B, acc_B = train_resnet(seed=99)"
   ],
   "id": "9463e65f8d37ade2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:04:59.380709Z",
     "start_time": "2025-06-12T20:04:59.377619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"ResNet-A clean-acc: {acc_A*100:.2f}%\")\n",
    "print(f\"ResNet-B clean-acc: {acc_B*100:.2f}%\")\n",
    "src_nets = [resnet_A, resnet_B]"
   ],
   "id": "e6c4183890af9019",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-A clean-acc: 93.97%\n",
      "ResNet-B clean-acc: 94.08%\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:14:02.740944Z",
     "start_time": "2025-06-12T20:14:02.295745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_finetuned_resnet(ckpt_path: Path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    net = models.resnet18(weights=None)\n",
    "    net.fc = nn.Linear(net.fc.in_features, 10)\n",
    "    net.load_state_dict(ckpt[\"state_dict\"])\n",
    "    net.to(device).eval()\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    print(f\"Загружена модель из {ckpt_path.name},  val-acc = {ckpt['val_acc']*100:.2f}%\")\n",
    "    return net\n",
    "\n",
    "seeds = [42, 99]\n",
    "net_names = [f'best_{x}.pth' for x in seeds]\n",
    "src_nets = [load_finetuned_resnet(MODELS_PATH / x) for x in net_names]"
   ],
   "id": "622bbdfb36c6568f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружена модель из best_42.pth,  val-acc = 93.97%\n",
      "Загружена модель из best_99.pth,  val-acc = 94.08%\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Создадим нашу основную модель, которую мы будем обучать\n",
    "\n",
    "Последний (линейный) слой классификатора заменим, чтобы он предсказывал один из 10 классов (в предыдущей работе за нас это делал ultralytics, здесь же это приходится делать вручную)\n",
    "\n",
    "Оптимизатор и планировщик так же раньше за нас добавляла библиотека, сейчас просто используем наиболее подходящие"
   ],
   "id": "b732f00007f32504"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:14:14.569157Z",
     "start_time": "2025-06-12T20:14:14.518166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "yolo = YOLO(\"yolov8s-cls.pt\")\n",
    "net  = yolo.model\n",
    "\n",
    "head = net.model[-1]\n",
    "in_f = head.linear.in_features\n",
    "head.linear = nn.Linear(in_f, 10, bias=True)\n",
    "net.to(device)\n",
    "\n",
    "opt  = torch.optim.AdamW(net.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, EPOCHS*len(dl_train))"
   ],
   "id": "3e809905e5646e97",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Напишем свой вариант FGSM-атаки. Очень хотелось бы использовать реализацию из ART, однако она может работать только на процессоре, и постоянные переводы с GPU на CPU существенно замедлят обучение",
   "id": "f3ba6e557aa07249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:15:49.320929Z",
     "start_time": "2025-06-12T20:15:49.315903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_logits(model, x):\n",
    "    out = model(x)\n",
    "    return out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "def fgsm(images, labels, src_model):\n",
    "    eps_norm = torch.tensor(EPS, device=device) / torch.tensor(std, device=device).view(3,1,1)\n",
    "    images = images.clone().detach().requires_grad_(True)\n",
    "\n",
    "    logits = get_logits(src_model.eval(), images)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    adv = images + eps_norm * images.grad.sign()\n",
    "    lo = (-torch.tensor(mean, device=device) / torch.tensor(std, device=device)).view(3,1,1)\n",
    "    hi = ((1 - torch.tensor(mean, device=device)) / torch.tensor(std, device=device)).view(3,1,1)\n",
    "    adv = adv.clamp(lo, hi).detach()\n",
    "    return adv"
   ],
   "id": "1a478a6b5777cc6a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Объявим ещё полезную функцию, которую будем вызывать каждые 10 эпох",
   "id": "c917b954136132df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T20:12:51.938750Z",
     "start_time": "2025-06-12T20:12:51.933245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_and_checkpoint(\n",
    "        epoch: int,\n",
    "        net: torch.nn.Module,\n",
    "        dl_test: torch.utils.data.DataLoader,\n",
    "        ds_test_len: int,\n",
    "        fgsm_fn,\n",
    "        ckpt_dir: Path,\n",
    "        opt: torch.optim.Optimizer,\n",
    "        sched,\n",
    "        best_robust: float,\n",
    "        get_logits_fn,\n",
    "        eps: float = 2 / 255\n",
    "    ) -> float:\n",
    "    net.eval()\n",
    "    clean_hits = robust_hits = 0\n",
    "\n",
    "    for x, y in dl_test:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clean_logits = get_logits_fn(net, x)\n",
    "\n",
    "        adv_x = fgsm_fn(x, y, net)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            adv_logits = get_logits_fn(net, adv_x)\n",
    "\n",
    "        clean_hits  += (clean_logits.argmax(1) == y).sum().item()\n",
    "        robust_hits += (adv_logits.argmax(1)  == y).sum().item()\n",
    "\n",
    "    clean_acc  = clean_hits  / ds_test_len\n",
    "    robust_acc = robust_hits / ds_test_len\n",
    "\n",
    "    print(f\"\\nEpoch {epoch:02d}: \"\n",
    "          f\"clean {clean_acc:.3f} | FGSM ε={eps} {robust_acc:.3f}\")\n",
    "\n",
    "    # ----------- сохранить обычный чекпоинт -----------\n",
    "    ckpt_path = ckpt_dir / f\"epoch_{epoch:02d}.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": net.state_dict(),\n",
    "        \"optimizer_state\": opt.state_dict(),\n",
    "        \"scheduler_state\": sched.state_dict(),\n",
    "        \"clean_acc\": clean_acc,\n",
    "        \"robust_acc\": robust_acc,\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint ➜ {ckpt_path.name}\")\n",
    "\n",
    "    # ----------- сохранить лучший ---------------------\n",
    "    if robust_acc > best_robust:\n",
    "        best_robust = robust_acc\n",
    "        best_path   = ckpt_dir / \"best.pth\"\n",
    "        torch.save(net.state_dict(), best_path)\n",
    "        print(f\"New best robust_acc = {best_robust:.3f} ➜ {best_path.name}\")\n",
    "\n",
    "    net.train()            # вернуться к обучению\n",
    "    return best_robust"
   ],
   "id": "da56d5acfaec5dc0",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Наконец, можем переходить к обучению",
   "id": "32bbe91859a6e174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T21:04:26.442193Z",
     "start_time": "2025-06-12T20:54:16.479838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ckpt_dir = Path(\"checkpoints_eat\")\n",
    "ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "best_robust = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    net.train()\n",
    "    running_acc = 0\n",
    "    pbar = tqdm(dl_train, desc=f\"Epoch {epoch:02d}/{EPOCHS}\", leave=False)\n",
    "\n",
    "    for step, (imgs, lbls) in enumerate(pbar, 1):\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "\n",
    "        # добавляем adv. examples\n",
    "        src = random.choice(src_nets)\n",
    "        k   = int(imgs.size(0) * ADV_FRACTION)\n",
    "        adv = fgsm(imgs, lbls, src)\n",
    "\n",
    "        mix_imgs   = torch.cat([imgs[k:], adv[:k]])\n",
    "        mix_labels = torch.cat([lbls[k:], lbls[:k]])\n",
    "\n",
    "        #  шаг оптимизации\n",
    "        opt.zero_grad()\n",
    "        logits = get_logits(net, mix_imgs)\n",
    "        loss   = F.cross_entropy(logits, mix_labels, label_smoothing=0.1)\n",
    "        loss.backward(); opt.step(); sched.step()\n",
    "\n",
    "        # обновляем прогресс бар\n",
    "        pred = logits.argmax(1)\n",
    "        running_acc += (pred == mix_labels).sum().item()\n",
    "        avg_acc = running_acc / (step * mix_labels.size(0))\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.3f}\",\n",
    "                 clean_acc=f\"{(pred[k:]==lbls[k:]).float().mean():.3f}\",\n",
    "                 mix_acc=f\"{avg_acc:.3f}\")\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == EPOCHS:\n",
    "        best_robust = validate_and_checkpoint(\n",
    "            epoch         = epoch,\n",
    "            net           = net,\n",
    "            dl_test       = dl_test,\n",
    "            ds_test_len   = len(ds_test),\n",
    "            fgsm_fn       = fgsm,\n",
    "            ckpt_dir      = ckpt_dir,\n",
    "            opt           = opt,\n",
    "            sched         = sched,\n",
    "            best_robust   = best_robust,\n",
    "            get_logits_fn = get_logits,\n",
    "            eps           = EPS\n",
    "        )"
   ],
   "id": "bd15b28ac8916e3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: clean 0.347 | FGSM ε=0.00392156862745098 0.148\n",
      "Saved checkpoint ➜ epoch_10.pth\n",
      "New best robust_acc = 0.148 ➜ best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: clean 0.348 | FGSM ε=0.00392156862745098 0.150\n",
      "Saved checkpoint ➜ epoch_20.pth\n",
      "New best robust_acc = 0.150 ➜ best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[70]\u001B[39m\u001B[32m, line 34\u001B[39m\n\u001B[32m     32\u001B[39m     running_acc += (pred == mix_labels).sum().item()\n\u001B[32m     33\u001B[39m     avg_acc = running_acc / (step * mix_labels.size(\u001B[32m0\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     \u001B[43mpbar\u001B[49m\u001B[43m.\u001B[49m\u001B[43mset_postfix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[33;43m.3f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m             \u001B[49m\u001B[43mclean_acc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m==\u001B[49m\u001B[43mlbls\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[33;43m.3f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m             \u001B[49m\u001B[43mmix_acc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mavg_acc\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[33;43m.3f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m epoch % \u001B[32m10\u001B[39m == \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m epoch == EPOCHS:\n\u001B[32m     39\u001B[39m     best_robust = validate_and_checkpoint(\n\u001B[32m     40\u001B[39m         epoch         = epoch,\n\u001B[32m     41\u001B[39m         net           = net,\n\u001B[32m   (...)\u001B[39m\u001B[32m     50\u001B[39m         eps           = EPS\n\u001B[32m     51\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1431\u001B[39m, in \u001B[36mtqdm.set_postfix\u001B[39m\u001B[34m(self, ordered_dict, refresh, **kwargs)\u001B[39m\n\u001B[32m   1428\u001B[39m \u001B[38;5;28mself\u001B[39m.postfix = \u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(key + \u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m + postfix[key].strip()\n\u001B[32m   1429\u001B[39m                          \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m postfix.keys())\n\u001B[32m   1430\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m refresh:\n\u001B[32m-> \u001B[39m\u001B[32m1431\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrefresh\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1347\u001B[39m, in \u001B[36mtqdm.refresh\u001B[39m\u001B[34m(self, nolock, lock_args)\u001B[39m\n\u001B[32m   1345\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1346\u001B[39m         \u001B[38;5;28mself\u001B[39m._lock.acquire()\n\u001B[32m-> \u001B[39m\u001B[32m1347\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1348\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m nolock:\n\u001B[32m   1349\u001B[39m     \u001B[38;5;28mself\u001B[39m._lock.release()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1495\u001B[39m, in \u001B[36mtqdm.display\u001B[39m\u001B[34m(self, msg, pos)\u001B[39m\n\u001B[32m   1493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pos:\n\u001B[32m   1494\u001B[39m     \u001B[38;5;28mself\u001B[39m.moveto(pos)\n\u001B[32m-> \u001B[39m\u001B[32m1495\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__str__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1496\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pos:\n\u001B[32m   1497\u001B[39m     \u001B[38;5;28mself\u001B[39m.moveto(-pos)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:459\u001B[39m, in \u001B[36mtqdm.status_printer.<locals>.print_status\u001B[39m\u001B[34m(s)\u001B[39m\n\u001B[32m    457\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprint_status\u001B[39m(s):\n\u001B[32m    458\u001B[39m     len_s = disp_len(s)\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     \u001B[43mfp_write\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\r\u001B[39;49;00m\u001B[33;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlast_len\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43mlen_s\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    460\u001B[39m     last_len[\u001B[32m0\u001B[39m] = len_s\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:453\u001B[39m, in \u001B[36mtqdm.status_printer.<locals>.fp_write\u001B[39m\u001B[34m(s)\u001B[39m\n\u001B[32m    451\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfp_write\u001B[39m(s):\n\u001B[32m    452\u001B[39m     fp.write(\u001B[38;5;28mstr\u001B[39m(s))\n\u001B[32m--> \u001B[39m\u001B[32m453\u001B[39m     \u001B[43mfp_flush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\utils.py:196\u001B[39m, in \u001B[36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    194\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minner\u001B[39m(*args, **kwargs):\n\u001B[32m    195\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m196\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    197\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    198\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m e.errno != \u001B[32m5\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\iostream.py:609\u001B[39m, in \u001B[36mOutStream.flush\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    607\u001B[39m     \u001B[38;5;28mself\u001B[39m.pub_thread.schedule(evt.set)\n\u001B[32m    608\u001B[39m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m609\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    610\u001B[39m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[32m    611\u001B[39m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[32m    612\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mIOStream.flush timed out\u001B[39m\u001B[33m\"\u001B[39m, file=sys.__stderr__)\n\u001B[32m    613\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:629\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    627\u001B[39m signaled = \u001B[38;5;28mself\u001B[39m._flag\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m     signaled = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cond\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    630\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:331\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    330\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m         gotit = \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    333\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Доля правильных ответов на комбинированной выборке не превышает 34%, что делает эксперимент неудачным\n",
    "\n",
    "В будушем можно к нему вернуться. Скорее всего, перед моделью была поставлена слишком тяжелая задача"
   ],
   "id": "b83b1bd47315ac73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
